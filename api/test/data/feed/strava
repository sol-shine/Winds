
<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[strava-engineering - Medium]]></title>
        <description><![CDATA[Engineers building the social network for athletes. - Medium]]></description>
        <link>https://medium.com/strava-engineering?source=rss----89d4108ce2a3---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>strava-engineering - Medium</title>
            <link>https://medium.com/strava-engineering?source=rss----89d4108ce2a3---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 09 Jun 2018 16:46:34 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/strava-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Apple Dev Guild Week]]></title>
            <link>https://medium.com/strava-engineering/apple-dev-guild-week-f5981fe525a4?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/f5981fe525a4</guid>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[swift]]></category>
            <category><![CDATA[ios]]></category>
            <dc:creator><![CDATA[Jeff Remer]]></dc:creator>
            <pubDate>Mon, 04 Jun 2018 23:55:00 GMT</pubDate>
            <atom:updated>2018-06-04T23:55:00.331Z</atom:updated>
            <content:encoded><![CDATA[<h4>Swift Adoption</h4><p>The team of developers that works on our iPhone and Apple Watch app spends the majority of its time working on feature development during the year. However, for a week in April the Apple Dev Guild had the opportunity to focus on platform health and the future of what writing iPhone and Watch apps looks like at Strava. Until this year writing iOS and WatchOS apps at Strava has meant writing entirely Objective-C. As a guild and as an organization we determined that it was important to begin our shift to Swift.</p><h3>Why Swift? Why now?</h3><p>Swift is a fast, safe, modern programming language. Swift was a fresh start for Apple. Unlike Objective-C, which comes with the baggage of being incrementally built on top of C, was designed from the ground up with features such as strict type safety and succinctness that help prevent programmer error, ease in maintenance, and let the programmer focus on logic and business rules.</p><p>Swift is the standard for writing iOS apps, partially because Apple declared it the official language, but also because it’s effective. Programmers are using it, it is the fastest growing programming language overall, and we’re at the stage where many iOS programmers have only ever written Swift. As a growing company we get candidates all the time that have only written Swift, which is perfectly fine with us since we tend to believe that being an effective iOS programmer is more about knowing the platform and having strong software engineering fundamentals. However, it was clear to us that in order to attract engineers we needed to evolve.</p><p>Strava’s iOS codebase is 7 years old and is almost entirely Objective-C. Swift has been around for years. Why hadn’t we really started adopting Swift until this year? At Strava we pride ourselves on adopting a healthy and reasonable mix of bleeding edge technologies and tried and true solutions. We actually did try out Swift in the early days, mostly experimentally, but every Swift release resulted in breaking API changes that just left a bad taste in our mouth. We witnessed several situations where other companies had to repeatedly rewrite their apps written in early versions of Swift and had a hard time coping with API changes across Xcode releases. As time went by and our codebase grew substantially we just simply didn’t risk making the switch, at least until the benefits started to clearly outweigh the disadvantages.</p><p>By the time Swift 4 was released it seemed like its maturity was commensurate with ours and we decided to dive fully in. As a guild we spent some time during the latter half of 2017 ensuring that we were all roughly on the same page as far as technical aptitude in Swift so that engineers could review each other’s code without getting completely lost. At the start of 2018 we decided that all new files should be written in Swift. We have a long way to go, but as of this writing our codebase is approximately 6% Swift and 94% Objective-C. That may not seem like much, but in a codebase of thousands of files and hundreds of thousands of lines of code going from zero Swift to just over 6% this year is a good start.</p><h3>Building up our infrastructure</h3><p>We started adopting Swift at the beginning of the year with enough in our toolbox to keep us working without slowing down product development, but we didn’t lay down a lot of ground rules. We trusted that the nature of Swift’s syntax and type safety would be enough to get us going and avoid the need to rely on manually checking a style guide in order to prevent us from shooting ourselves in the foot. We did integrate <a href="https://github.com/realm/SwiftLint">SwiftLint</a> early on and have adjusted the set of rules we use over time to suit our needs. Developers report that they are pleased that an impartial robot tells them something is wrong with their code at compile-time, rather than getting nitpicked by their colleagues in pull requests.</p><h3>Xcode 9.3 and Swift 4.1 Update</h3><p>Guild week coincided nicely with the release of Xcode 9.3 and Swift 4.1. We spent some time ensuring that our existing Swift code was Swift 4.1 compatible and we updated our project settings to take advantage of the language update.</p><h3>Swift dependency injection with Swinject</h3><p>We’ve used <a href="http://appsquickly.github.io/Typhoon/">Typhoon</a> as our dependency injection framework on iOS for many years and it’s served us well. Although Typhoon supports Swift, we’ve viewed our Swift adoption as an opportunity to try out new things. During guild week we gave <a href="https://github.com/Swinject/Swinject">Swinject</a> a try as a lightweight, pure Swift alternative to Typhoon.</p><h3>BDD with Quick and Nimble</h3><p>Behavior driven development offers an appealing refinement to TDD that can help organize thinking around testing the behavior of your application, especially when compared with traditional unit testing practices. BDD is used widely by the Web guild and we have used both <a href="https://github.com/cedarbdd/cedar">Cedar</a> and <a href="https://github.com/kiwi-bdd/Kiwi">Kiwi</a> in the past at Strava in Objective-C, but we kept returning to regular XCTestCases. BDD in Swift regains some of the elegance lost in BDD in Objective-C, so we decided to give it another shot by integrating <a href="https://github.com/Quick/Quick/">Quick</a> and <a href="https://github.com/Quick/Nimble">Nimble</a>. Quick is similar to RSpec in that it has helped us write more understandable and thorough tests.</p><h3>Miscellany</h3><p>By the time guild week had started in April we were already habitually writing Swift, so the entire focus that week wasn’t just on further Swift adoption. We had a growing laundry list of improvements we hoped to make across the app and across our development toolchain, so we set out to accomplish some of them while we could devote a relatively large chunk of time.</p><p>Some of these improvements included:</p><ul><li>Migrating from HockeyApp to Crashlytics</li><li>Automating integration of internal Cocoapod updates</li><li>Updated our iPhone recording code to use the modernized Strava for Apple Watch recording code</li><li>Added flexibility to our internal URL routing mechanism</li></ul><h3>Looking forward</h3><p>Strava’s second guild week is on the horizon and the Apple Dev Guild is taking some inspiration from the Android guild’s modularization efforts. We’re going to largely focus on modularizing our codebase further with the goals of establishing an architecture that is easier and less brittle to work with. We have some of this architecture in place already and it’s shown to help make working in a large code base feel small and manageable again. Aside from a qualitative improvement in coding confidence we want to establish empirically that these efforts are worth our time. We’ll begin tracking and analyzing build and test times. Alongside our Swift adoption our hope is that a more modular architecture leads to an more stable, more testable, and faster app.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f5981fe525a4" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/apple-dev-guild-week-f5981fe525a4">Apple Dev Guild Week</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Time to Learn React!]]></title>
            <link>https://medium.com/strava-engineering/time-to-learn-react-e207ca12ad57?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/e207ca12ad57</guid>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[rails]]></category>
            <category><![CDATA[react]]></category>
            <dc:creator><![CDATA[Pan Thomakos]]></dc:creator>
            <pubDate>Mon, 21 May 2018 16:33:50 GMT</pubDate>
            <atom:updated>2018-05-21T16:33:50.831Z</atom:updated>
            <content:encoded><![CDATA[<h3>Web Guild Week at Strava</h3><p>The web guild at Strava is responsible for maintaining and improving the Ruby/Rails/JavaScript/CSS platform, processes, and culture. We believe that an up-to-date, modern web platform is not only more secure and faster, but also makes our engineers more productive and happier. As part of a company wide <a href="https://medium.com/strava-engineering/guild-week-at-strava-8edc1d7b5274">Guild Week</a>, during the week of April 2nd all of our web engineers took time off from their regularly scheduled product work to focus solely on learning React.</p><h3>jQuery, Backbone, and CoffeeScript — oh my!</h3><p>Strava’s main monolithic Rails application began its life over 9 years ago. At that time functional and immutable JavaScript was not in vogue, libraries like React and Redux did not exist, and CSS modules were not a thing. In the early days of Strava we used jQuery and over time, as our application grew, we adopted CoffeeScript and Backbone. Since then, our JavaScript stack has fallen behind the times. Compared to new versions of the Javascript language and new front-end application paradigms, our Javascript stack is brittle and difficult to work with or feel excited about. Near the end of last year we decided to experiment with a few of the newest front-end languages and libraries in order to select a set of tools that would allow us to continue to meet our product goals and stay nimble and reactive to the latest advancements in front-end development.</p><p>We developed a simple rubric for evaluating our options and then selected some engineers to split off into teams and develop a proof of concept login view using each of the technologies we were evaluating. We then came together at the end of the experiment, discussed our findings, and, as you may have guessed, ended up choosing React on ES6.</p><p>At the time of this decision there were only a handful of web engineers that conceptually understood React and we hadn’t made many of our most important technical and best practice decisions yet. How were we going to test our code? Would we be using Redux? Were we going to use CSS modules? How would we integrate with existing code and our existing Rails application? How would we go about migrating our old CoffeeScript code over to React? How would a new engineer at Strava become proficient in React and its associated technologies?</p><h3>Exploring React-land</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*y6C4nSvy2Woe0m7bWEn4BA.png" /><figcaption><a href="https://commons.wikimedia.org/wiki/File:React_Native_Logo.png">React Logo</a> / <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Creative Commons</a></figcaption></figure><p>Over the next few weeks a group of Strava engineers, dubbed “The React Working Group”, integrated React into our <a href="https://strava.com/local">Strava Local</a> Rails app. This app is independent of our monolith and was a good candidate for continuing to develop our ideas and understanding of this new set of tools. During that time we answered some of the most basic questions and developed some initial best practices. We were going to use <a href="https://facebook.github.io/jest/">Jest</a> for testing, we were not going to use Redux yet, <a href="https://github.com/css-modules/css-modules">CSS modules</a> were a go, and we would upgrade our apps to Rails 4.2 in order to use the <a href="https://github.com/rails/webpacker">Webpacker Gem</a> for easy integration with <a href="https://webpack.js.org/">webpack</a>.</p><p>The complicating factor was that our main Rails application (<a href="http://www.strava.com">www.strava.com</a>) was running Rails 3.2 so we needed to make a very concerted effort to speed up our upgrade plans. I can write a whole other blog post series on upgrading our Rails application, but we’ll stick with React for this post. The summary is that we did it. The Rails 4.0 upgrade was by far the most difficult part and took over a month to complete. The 4.1 and 4.2 upgrades each took about two weeks and included backwards-compatible development work and deployment. We did not have to roll-back any deploys so we considered this a massive success.</p><style>body[data-twttr-rendered="true"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true"><p>Rails 4.0 and 4.1 upgrades rolled out within the last two weeks from the <a href="http://twitter.com/StravaEng" target="_blank" title="Twitter profile for @StravaEng">@StravaEng</a> web engineers. Time for a celebration and onwards to Rails 4.2!</p><p>&#x200a;&mdash;&#x200a;<a href="https://twitter.com/panthomakos/status/973722555101061120">@panthomakos</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute("width")); if ( 500  < maxWidth) {window.frameElement.setAttribute("width", "500");}}</script><h3>Establishing a Permanent Colony</h3><p>At this point we felt that we had a sufficient starting base from which we could begin adopting React as our primary frontend framework. The problem, of course, was that we had a lot of engineers that knew very little about ES6, React, Jest, Webpack, and CSS modules — let alone how all of these new tools would interact with our existing Rails app.</p><p>Enter guild week. Guild week was a perfect opportunity to spend 100% of our time focused entirely on learning these new tools, continuing to develop our best practices, and building a knowledge base so that future employees would be able to learn to use React at Strava quickly. The goal at the end of this week was not to have some percentage of our main Rails app rewritten in React. Instead, we wanted every engineer in the web guild to understand React code and concepts and feel comfortable reviewing ES6 and JSX. The ideal outcome was that going forward the entire guild would agree that all new JavaScript code would be written in React and that over time we would refactor existing backbone views and models into React.</p><p>We spent the first few days of the week working through the <a href="https://reactforbeginners.com/">Web Bos React for Beginners</a> course. This got everyone up to speed with basic ES6, JSX, and React. We then spent the rest of the week writing actual code, comparing results, and learning some more advanced concepts. There was no pressure to produce production level code. If it took one engineer longer to work through the basic React course than another engineer, that was completely okay.</p><p>Throughout the week we also organized some hour-long presentations from the React Working Group members. These sessions were recorded so that future employees could follow along and get caught up. For example, we organized one session on Jest/Testing and another on CSS Modules.</p><p>When we actually dove into code we constrained ourselves to a set list of some smaller views/projects and purposefully duplicated each others work. We wanted to be able to compare implementations so that we could learn and develop consistent practices going forward.</p><h3>Are We There Yet?</h3><p>React week was a great success. The team benefited greatly from being able to take a full week off regular work and just learn. Ultimately this dedicated learning time will result in long term productivity, reliability, and confidence gains. Even if all of us don’t immediately go off and start writing React components, we will be able to more confidently review code and assess technical tradeoffs as we continue to build out our tooling and infrastructure.</p><p>There are some things we could have done better. We should have done more to plan and organize our learning sessions and we should have done a better job of setting up some of the basic tooling and integrations with our web app ahead of time. Part of the reason we weren’t as prepared is that we had just finished our Rails 4.2 upgrade and had not had time to do as much setup as we would have liked.</p><p>In the following weeks we have made additional progress. We have a full “Getting Started in JavaScript” guide which summarizes each of the tools and best practices we have adopted — along with links to the recorded guild week presentations. We have fully integrated linting and testing of JavaScript and CSS into our CI pipeline. And we continue to make improvements to our deployment and development pipelines: for example by de-duping and better organizing our production JavaScript assets.</p><p>We’re excited for the future of JavaScript at Strava! And we’re really looking forward to the next Guild week. If you’re interested in joining the web guild and getting to participate in the next two web guild weeks we’ll have this year, we are always hiring web engineers. Check out our open positions <a href="http://bit.ly/2p4YRuY">here</a> or reach out to me directly <a href="https://twitter.com/panthomakos">@panthomakos</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e207ca12ad57" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/time-to-learn-react-e207ca12ad57">Time to Learn React!</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Android Guild Week: Modularizing our App]]></title>
            <link>https://medium.com/strava-engineering/android-guild-week-924ed49b668c?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/924ed49b668c</guid>
            <category><![CDATA[android]]></category>
            <category><![CDATA[mobile]]></category>
            <dc:creator><![CDATA[Todd Santaniello]]></dc:creator>
            <pubDate>Mon, 30 Apr 2018 22:57:26 GMT</pubDate>
            <atom:updated>2018-04-30T23:13:54.709Z</atom:updated>
            <content:encoded><![CDATA[<p>The Android team at Strava tries to maintain a healthy level of refactoring in conjunction with feature development throughout the year. However, earlier this month, all Android engineers had the opportunity to focus solely on platform health for an entire week (see our post about <a href="https://medium.com/strava-engineering/guild-week-at-strava-8edc1d7b5274">Guild Weeks</a> for the history). After reviewing potential areas on which to focus, we decided to spend the week dividing our codebase into functional modules. This post reviews our reasoning, approach and outcomes.</p><h3>Why create more modules?</h3><p>The Android codebase is approximately 6.5 years old and started, as most projects do, with a single module (now named handset). Over the years we added a few new modules, but the majority of our code was still concentrated in handset. With improvements to the Android Gradle Plugin (AGP) such as build caching by module and parallel compilation, we set out to modularize our codebase with the following goals:</p><ul><li>make the project more readable and less tightly coupled</li><li>increase build speed</li><li>enable more efficient development iterations (explained later)</li></ul><h3>Prerequisites</h3><p>Leading up to Guild Week, we agreed it was important to establish some benchmarks and build tools to track our progress. To address this, we added 2 items to our gradle configuration:</p><ol><li>time-based tracking of build tasks — we used <a href="https://github.com/nebula-plugins/gradle-metrics-plugin">Netflix’s Nebula plugin</a> to achieve this</li><li>simple code distribution counts — we wrote a basic gradle task that runs <a href="https://github.com/AlDanial/cloc">cloc</a> locally to give a snapshot of distribution across modules</li></ol><h3>Defining the work</h3><p>We follow a Scrum process at Strava, so we wrote stories to extract different pieces of our app into new modules. Some examples include:</p><ul><li>Create a Challenges module — the Challenges feature is fairly self-contained and seemingly easy to extract</li><li>Create a Test Utilities module — we had some test code duplicated across modules and it made sense to move it to a common location that was reusable by all</li><li>Create an Authorization module — if we extract the login flow to a separate module, engineers can assemble <em>mini-apps</em> during feature development that allow focused iteration. For example, an engineer building new views for our Clubs module could deploy a “Clubs” app that consists of the authorization, clubs and core modules. This <em>should</em> build and deploy in a fraction of the total handset build.</li></ul><p>With our benchmarks established and a week ahead of us, we divvied up the stories and charged forward.</p><h3>One step at a time</h3><p>We all started by creating new, empty modules and then proceeded to extract code out of handset into the new modules. This is also when we really started to shine a light on years of accumulated complexities. Two examples highlight the challenges we faced:</p><h4>Authorization</h4><p>As mentioned earlier, we knew creating an authorization module would be a big win. The path to extract that code unfolded with the following steps:</p><ul><li>getAthlete() — In our codebase, the authorization process revolves around the Athlete class. In the past year, we had already moved a lot of Athlete code from our handset module to an athlete module, but had not yet extracted the fundamental getAthlete() method that queries our API, which is a key step in the authorization flow.</li><li>Push notification settings coupling — When we tried to extract getAthlete(), we saw that the completion callback method syncs push notification settings to our API. Unfortunately, our notification settings code resided in handset and was tightly coupled with several classes there. So we set out to create a notifications module and extract that.</li><li>Pencils down — the week ended while we were in the midst of extracting the notification code to a new module. As of this writing, we are finally ready to land those changes.</li></ul><p>While we slowly moved the authorization code following the process above, we also built an interim solution to achieve mini-apps:</p><ul><li>We added an option to our development menu (not shipped in production builds) that exports preferences and authorization information.</li><li>We introduced a new launcher module that consumes the exported preferences and runs a given starting Activity.</li><li>This allows engineers to deploy a local development app with a subset of the desired modules and authorized access to our test servers.</li></ul><h4>View complexities</h4><p>Another issue we encountered during the week was untangling legacy Activity and Fragment implementations. Long ago, we added StravaBaseActivity and StravaBaseFragment classes. They started as simple ways to provide things like lifecycle method performance tracking and basic injection of commonly used interfaces. Over time, these classes grew well beyond their scope and violated the OOP principle of <em>Composition over Inheritance</em>. We have been gradually migrating our view implementations to an MVVM pattern, but during Guild Week we discovered many of the legacy Activities and Fragments were effectively trapped in the handset module due to the extensive dependencies on Base classes. To address this, we migrated what Views we could and made plans to provide much lighter Base classes in a common-ui module which will ease the transition out of handset.</p><h3>Moving forward</h3><p>Aforementioned complexities aside, we continued our work toward a more modular codebase and made solid progress throughout the week. The team banded together, reviewing Pull Requests and socializing our process to determine how, when and why we introduce a new module. We also ensured a README file accompanied every module that describes its function and assigns ownership to engineers with expertise.</p><p>In addition, we improved our code on several other fronts:</p><ul><li>Since we recently increased our minSdkVersion, we were able to move our images to webp and reduce our APK footprint.</li><li>We deleted a lot of unused build configurations and code from app flavors we had deprecated, simplifying our overall gradle structure.</li><li>We consolidated all of our string content to a single module. This drastically reduced the complexity in our translation process and simplified the steps necessary to create a new module.</li><li>We successfully created a test-utils Java library module. This consolidated all of our shared test classes to a single module that we can now include with a simple testImplementation project(&#39;:test-utils&#39;) line in any gradle file.</li></ul><p>On Monday, we re-ran our tracking tools to measure the efforts and assess our week.</p><h3>Drawing Conclusions</h3><p>We arrived at the following conclusions when the week was over:</p><h4>Modules and code distribution</h4><p>We introduced 8 new modules and reduced the percentage of the codebase in handset from 77% to 74%. 3% does not seem like a large amount, but in one week we established momentum behind modularization and arrived at the following general process: <em>move things, see what breaks, untangle, repeat.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/602/0*-u8Iagd2e3TzR8Jq." /><figcaption>Subset of code distribution metrics by module before &amp; after Guild Week</figcaption></figure><h4>Build Speed</h4><p>Continuous Integration (CI) build times actually increased by 23%. At first this was alarming, as one of the main reasons we began this process was to decrease time spent waiting for our code to build. However, we dug into the numbers further and discovered that due to the changes in the codebase throughout the week, our CI image was spending a lot of time downloading newly-added dependencies. To address this, we increased the frequency at which we rebuild our CI image and continue to monitor build speed closely. We are also investigating a process to automatically rebuild the image every time new dependencies are introduced, eliminating noise from the speed measurements.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/588/0*M5Mc9XQSWbZVRGm9." /><figcaption>Elapsed build times (pink line) during Guild Week</figcaption></figure><p>In addition, our interim solution for <em>mini-apps</em> yielded great results regarding build speed. While not tracked in our CI environment, tests on development machines showed a 4x reduction in build time when deploying a mini-app vs deploying the complete handset APK. This will dramatically speed up the development iterations engineers encounter on a daily basis.</p><p>Overall, the team had a great time working together towards a cleaner codebase. We’re actively tracking our progress and are eager to see how much we can achieve before our next Guild Week.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=924ed49b668c" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/android-guild-week-924ed49b668c">Android Guild Week: Modularizing our App</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Quantifying Effort through Heart Rate Data]]></title>
            <link>https://medium.com/strava-engineering/quantifying-effort-through-heart-rate-data-e6a0e3dd6a52?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/e6a0e3dd6a52</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[fitness]]></category>
            <dc:creator><![CDATA[Will Meyer]]></dc:creator>
            <pubDate>Tue, 17 Apr 2018 15:33:14 GMT</pubDate>
            <atom:updated>2018-04-17T16:45:28.605Z</atom:updated>
            <content:encoded><![CDATA[<p>From Suffer Score to Relative Effort</p><p>Written by: <a href="https://medium.com/u/690f2bb42297">Chris Spada</a> and <a href="https://medium.com/u/b8f957ed1f5">Will Meyer</a></p><p>One of our most popular Premium features is Suffer Score, a way to quantify effort during an activity using heart rate data. Although Suffer Score has become a popular feature for providing insights into an activity, it had limitations in providing accurate results across sports and athletes for similar efforts. The main problems were:</p><ol><li>The model wasn’t well suited for comparing effort across different sports. Running efforts were scored higher than rides and swims of the same length and intensity. This meant that a similar amount of effort applied to a run would result in a higher Suffer Score than it would have in a ride of a similar duration.</li><li>Length of the effort was weighted much more than intensity. Shorter, more intense efforts had lower Suffer Scores than longer, less intense ones even if the shorter effort had a much greater training effect.</li></ol><p>Working with Dr. Marco Altini, we developed a model to generate a new version of Suffer Score, called Relative Effort. This powers new Premium features that provide insight into week-over-week training load. The new model follows the same approach of using time spent in different heart rate zones to score an activity, but differs in a few key ways: it weights intensity of effort more than duration, is better at weighting equivalent efforts across sports, and scores equivalent efforts by different athletes similarly. The result is that Strava athletes are better able to use this metric to compare activity effort across workout types, sport types and even individuals.</p><p><strong>Tuning the Model</strong></p><p>To calculate Relative Effort, the model takes in a stream of data with an athlete’s heart rate and the corresponding timestamp as inputs. Using the athlete’s max heart rate (either entered by the athlete or estimated), this heart rate data is divided into a number of zones that approximate different levels of cardiovascular intensity. For each heart rate zone, the model applies a coefficient to weight the time spent in that zone. The higher the heart rate zone, the harder the effort, and a higher coefficient to more heavily weight time spent in that zone.</p><p>To achieve our goal of providing similar values for athletes producing similar efforts, we leveraged Strava’s rich data set to find a group of activities from athletes giving a roughly equivalent effort. To tune on a subset of activities from thousands of different athletes, we made the assumption that Athlete A giving an all-out effort should expend an equal amount of stress/effort as Athlete B, even if the athletes have different absolute fitness levels. These two athletes should then have similar Relative Effort values. After testing 5k, 10k, and half marathon distances, we settled on using 10k running race efforts to tune the coefficients applied to each zone. Running removes much of the variability associated with sports like cycling, where drafting and coasting can result in significant changes in effort and heart rate. The 10k distance is short enough that an athlete is consistently in a high heart rate zone, yet long enough for heart rate data to settle into a consistent pattern and avoid the noise often found in shorter events.</p><p>With a large subset of 10k running races on Strava, we iterated through thousands of different coefficient combinations for each heart rate zone, with the goal of minimizing variance of Relative Effort values from our data set. By minimizing the variance on a set of equivalent efforts, we sought to increase the precision of the model and rate equivalent efforts equally. We used the coefficient of variation (standard deviation / mean) to measure variance in order to control for the effect of increasing values when testing higher coefficients. We were able to decrease the coefficient of variation from 0.44 from the original coefficients to 0.39, a 12% improvement. This decrease in variance will improve the accuracy of the Relative Effort model and give our athletes a better representation of their efforts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ej9UgBULD02JGq3x." /></figure><p><strong>Extending the model to different sports</strong></p><p>The second objective of rebuilding Relative Effort was to better approximate efforts across activities. To do this, we used activities from Olympic distance triathlons as a way to find equivalent efforts to tune the model across sports. Although not perfect in terms of athletes giving equal effort in each leg, we think that in aggregate those activities should give us a reasonably hard effort for comparison. This was particularly valuable to validate using a lower set of heart rate zones for cycling activities, and a lower max heart rate for swim activities.</p><p>To make Relative Effort scores comparable between run and ride, we compared running and cycling activities from the Olympic distance triathlon dataset to view an approximation of the values using the new ride zones. The ride zones are relatively lower than run, as cycling heart rates are lower than weight-bearing activities like running. We found a median score of 211 for run, and 190 for ride. Even though the ride median is slightly lower than the run median, we’re comfortable with this because a cycling effort during a triathlon is usually submax to save some energy for the run.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZRizQwcbWTMmeZTa." /></figure><p>We wanted to make sure we had a good approximation between ride and run before we moved on to swimming. Heart rate based training isn’t as common for swimming, and literature for swim heart rate zones isn’t as well defined as run and ride. Additionally, swimming max heart rate has been described as 10–12 beats per minute lower than other activities. Given the non-weight-bearing nature of swimming, we used cycling zones and tested a mix of lower max heart rates and a lower lactate threshold heart rate. We settled on using cycling heart rate zones with a max heart rate 12 beats lower than the recorded value, based on comparing the Relative Effort distributions from swim, ride, and run legs of the triathlon data set.</p><p>For Strava’s 20 other supported activity types, there isn’t extensive research on relative heart rate differences between activities, so we assigned the activities either run or ride HR zones. This is roughly based on whether the sport is weight-bearing and bipedal (Nordic ski, hike) in which case we assigned the run zones, or is not weight-bearing (Windsurf, handcycle, kayak) and has lower expected heart rates, and applied the cycling zones.</p><p>Leveraging Strava’s extensive activity dataset, we were able to tune and extend Relative Effort to better quantify Strava athletes’ efforts, in turn powering a compelling new product for our athletes to help guide their training.</p><p><em>Thanks to Tommy Gaidus, Ethan Hollinshead, Kyle Yugawa, Varun Pemmaraju, and J Evans for their significant contributions and guidance on this project.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e6a0e3dd6a52" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/quantifying-effort-through-heart-rate-data-e6a0e3dd6a52">Quantifying Effort through Heart Rate Data</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Guild Week at Strava]]></title>
            <link>https://medium.com/strava-engineering/guild-week-at-strava-8edc1d7b5274?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/8edc1d7b5274</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[culture]]></category>
            <dc:creator><![CDATA[Chris Donahue]]></dc:creator>
            <pubDate>Thu, 12 Apr 2018 00:18:59 GMT</pubDate>
            <atom:updated>2018-04-12T16:17:09.126Z</atom:updated>
            <content:encoded><![CDATA[<p>Recently, the Strava Engineering team took a weeklong break from regular product work to focus attention on our core technology platforms. We coined this effort Guild Week. Before explaining Guild Week, it’d be useful to understand what a “Guild” is at Strava.</p><h3>A Brief History of Guilds at Strava</h3><p>Like most software companies, Strava started off with a single product development team, consisting of engineers, product managers, and designers. Engineers and designers were “free agents”, and worked on whatever project was highest priority. Eventually, we reached a point where that single team became unwieldy to manage. Product managers couldn’t plan for the medium / long term because they couldn’t count on having engineering and design support. Engineers couldn’t develop expertise in specific areas of the product, and also didn’t have the time to make longer-term investments in more complex product features.</p><p>To solve this problem, we split product development into what we called “vertical teams” — smaller teams comprised of mobile and web engineers, designers, analysts, and a product manager that each focused on one aspect of Strava’s product. These teams are relatively stable — we have had Growth and Premium vertical teams for over two years.</p><p>Of course, any team structure represents a compromise. In our case, the vertical team structure split iOS, Android, and Web engineers across multiple teams, which made it difficult to maintain consistency and cohesion across technology platforms. To counter this problem, we created a secondary team structure called Guilds. Each iOS, Android, and Web engineer became a member of their respective Guild in addition to their vertical team.</p><p>The Guilds are responsible for their technical platform: onboarding new engineers in that platform; keeping the platform current with the latest technologies; maintaining consistent design patterns across each codebase; and teaching one another about best practices.</p><h3>Guild Work</h3><p>We try to spend 10% of engineering time working on Guild projects. Tackling larger Guild projects is challenging when each engineer in the Guild is also responsible for projects on their vertical team. Guild work is of strategic importance to Strava — a healthy codebase enables us to build great products, and keeping our technology stack up-to-date ensures that our engineers are developing skills in modern systems and frameworks. As a result, we decided to supplement the 10% ongoing time with a series of Guild Weeks scheduled throughout the year. During a Guild Week, all engineers devote 100% of their time to their respective platforms. In this blog post series we’ll explore what each of our Guilds did during their first Guild Week of 2018.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8edc1d7b5274" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/guild-week-at-strava-8edc1d7b5274">Guild Week at Strava</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Diversity within Strava’s Engineering Organization]]></title>
            <link>https://medium.com/strava-engineering/diversity-within-stravas-engineering-organization-a48d0a0dd9ed?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/a48d0a0dd9ed</guid>
            <category><![CDATA[diversity]]></category>
            <dc:creator><![CDATA[Dickson Lui]]></dc:creator>
            <pubDate>Fri, 23 Feb 2018 18:42:55 GMT</pubDate>
            <atom:updated>2018-03-05T23:54:08.648Z</atom:updated>
            <content:encoded><![CDATA[<p><em>with contributions from Debbie Ly and the EDIG team</em></p><p>At Strava, we believe that building a diverse and inclusive culture is crucial to bringing out the best in our team.</p><ul><li>Creating a safe space fosters a sense of belonging, where all teammates can be more comfortable sharing their ideas and perspectives.</li><li>Fostering an environment of inclusion will continue to encourage strong candidates to apply to Strava, expanding our potential talent pool.</li><li>Creativity is an essential component of engineering work. A diverse set of experiences and backgrounds expands the knowledge base we draw from, and ultimately inspires inventive solutions.</li><li>We will deliver a better product to our athletes if our engineering team more closely matches the world demographic.</li></ul><p>These values have always been important to us — a group of volunteers created the original Diversity and Inclusivity Group (DIG) in 2015 to begin advocating for these issues. However, as our company grew larger in size, we needed more firepower to drive all of the improvements that we were constantly discovering. In mid-2017, we formed our Engineering Diversity and Inclusivity Group (EDIG) with a focus of attracting and engaging diverse engineering candidates and employees.</p><p>The first initiative of our group involved expanding our talent search to new, diverse talent pools. For example, one of our engineers talked to students at Harvey Mudd about job opportunities at Strava, where over 50% of the computer science graduates are women. Our engineers have also been working with <a href="http://www.code2040.org/">Code2040</a>, an organization focused on creating engineering opportunities for black and latinx students, to mentor fellows from underrepresented groups in the technology industry.</p><p>However, the process of interviewing at technology companies can often be stressful for candidates from non-traditional backgrounds. To combat this, we are beginning to send an informative packet to all candidates before their first interview. The goal is for candidates from any background to start the interview process feeling informed and prepared. Because we believe that having teams with diverse skill sets adds value to our engineering projects, we ask candidates questions that gauge their potential value to Strava outside of purely technical contributions. In doing so, we are able to draw a more holistic picture of what a candidate will add to their prospective team.</p><p>Hiring and retention are only the first steps towards our vision of a truly inclusive environment throughout our engineering organization. On an ongoing basis, EDIG members host diversity lunches and actively encourage all employees to join in. During these sessions, participants break out into small groups and engage in a safe conversation around current diversity issues. To incorporate external viewpoints in our way of thinking, we invite prominent speakers who have worked on major diversity initiatives at other technology companies. By encouraging these conversations and creating a safe space for employees to hear out new perspectives and ideas, we can foster a workplace where all voices are heard.</p><p>EDIG encourages all engineers within our organization to actively pursue diversity and inclusion. Traditionally, the challenge engineers face when participating in committees outside their core function is that they feel like they are taking time away from their day-to-day projects to work on something that is not universally seen as directly advancing their engineering career. At Strava, we explicitly encourage and support all engineers to dedicate time and energy to make Strava a better company in any way they see fit, including participating in diversity groups and attending diversity/inclusion conferences and talks. To start, we have seen more people throughout our organization gathering diversity data, scheduling lunchtime talks on important issues, and improving other parts of our hiring process.</p><p>From our internal surveys, we see an 18% year-over-year increase on the belief that Strava values diversity. In the past year, we’ve hired ten women and nine people of color to join our team. We have made good progress in 2017, but our team continues to explore other ways we can further improve. Strava is committed to furthering these principles and building a diverse team in 2018 and beyond. Stay tuned for more updates and learnings from Strava’s EDIG.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a48d0a0dd9ed" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/diversity-within-stravas-engineering-organization-a48d0a0dd9ed">Diversity within Strava’s Engineering Organization</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upgrading Kafka at Strava]]></title>
            <link>https://medium.com/strava-engineering/upgrading-kafka-at-strava-fda64bed0504?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/fda64bed0504</guid>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[internships]]></category>
            <category><![CDATA[kafka]]></category>
            <dc:creator><![CDATA[Danny Schofield]]></dc:creator>
            <pubDate>Mon, 18 Dec 2017 22:34:18 GMT</pubDate>
            <atom:updated>2017-12-19T19:30:14.801Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>About Me:</strong></p><p>My name is Daniel Schofield and I am a Senior at the University of Maryland. I have had the pleasure of interning at Strava on the Infrastructure team for the past 3 months. While I am an avid runner that has used Strava for a while, my interest in working here was also due to my interest in seeing how Strava engineering projects work under the hood.</p><p><strong>Interning:</strong></p><p>For the first couple weeks of my internship, I was assigned several different tasks to familiarize myself with Strava’s codebase. The biggest of these projects was making updates to the matched runs service, <a href="https://blog.strava.com/how-many-matched-runs-you-have-9021/">Toucan</a>. This was a great start to my internship. I could immediately see the product impact my changes made, while becoming more comfortable at Strava. I was then assigned my main project — implementing a new system for publishing events with <a href="https://kafka.apache.org/documentation/">Apache Kafka</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/415/0*cLvG3yeVLtuFvdW4." /></figure><p><strong>Project Background:</strong></p><p>Kafka is a distributed streaming platform, that lets you publish and subscribe to a stream of records.<strong> </strong>Kafka is heavily used at Strava — it logs the majority of events that happen on the platform, such as changes to activities, segment efforts, and kudos. It also plays an important role in logging client behavior events.</p><p>When I arrived at Strava, the majority of events were produced by our Rails front-end that serves API and web traffic. We wanted to improve how we published events to Kafka. Most importantly, we were producing events using an unmaintained Kafka client — <a href="https://github.com/bpot/poseidon">poseidon</a>. In addition, as Strava has grown, we have seen a growth in services that want to produce events to Kafka. When upgrading Kafka versions, the broker needs to be updated, followed by the clients. As the number of producers grew, it became more tedious to make all of the associated upgrades. Event logging is very important to many pieces of Strava’s products and we could not tolerate a loss of events, even when updating our logging system.</p><p>Theoretically it is easy to upgrade Kafka brokers, but it in practice it can be difficult. For example, attempting to upgrade the Kafka brokers, while supporting older clients, resulted in an increased web error rate — the Kafka client we were using, poseidon, did not handle reduced availability scenarios well. We also wanted be able to use the Kafka official API in Java. Overall, when I arrived there were a multitude of reasons to restructure the architecture of event logging to Kafka.</p><p><strong>The Solution:</strong></p><p>Several solutions to improve the way events are logged were explored, however, a service as the solution became clear. By putting event logging behind a service, we were able to leverage Thrift client code already in the current Rails codebase. This code easily allows for retries and other settings to be configured when sending requests to the service.</p><p>The design of the service focused on creating a single Kafka producer that exposes a Thrift endpoint. Rails first forms a request that includes a batch of events. When the service receives these events, it logs them to Kafka. This architecture is simple and allows for the Java Kafka client to be used when publishing. Additionally, any Thrift client can be used to publish events. Finally, having a single producer makes it easier to update the Kafka version and configuration — with a single event sink.</p><p>Through this standardization of a service, Kafka is accessible to any new services that want to produce events with a simple Thrift request, as opposed to having to handle Kafka client configuration. This service can be enhanced to handle extra features such as message validation and any additional logic that could be introduced in the future. The service was then deployed using <a href="https://mesosphere.github.io/marathon/">Marathon</a> to be able to leverage other pre-existing infrastructure such as autoscaling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4Ihwg-XXgKi28XHU." /></figure><p><strong>Lessons Learned:</strong></p><p>Being able to own the development of my own service was helpful for growth as an engineer. Creating my own service allowed me to learn about the open source code that we use at Strava. For example, the application is deployed on <a href="https://mesosphere.github.io/marathon/">Marathon</a> with Docker containers, which gave me insight into container orchestration. Strava also uses <a href="https://linkerd.io/">linkerd</a> and <a href="http://zookeeper.apache.org/">Zookeeper</a> for load balancing and service discovery, in addition to using <a href="https://graphiteapp.org/">Graphite</a> for metrics collection. One of the best things about developing this service was that I was able to gain a sense of how these different technologies interact with one another. Working at Strava, I’ve had the opportunity to learn about technologies I didn’t know existed before I arrived.</p><p><strong>Overall Experience:</strong></p><p>Interning at Strava was an amazing experience and I am extremely thankful that I had the opportunity to work here. I was able to learn about software and best practices in the industry. It was great to see that even though people have strong opinions on the product, they are still able to make data-driven decisions that focus on what is best for the athlete. While here I also had the opportunity to run my first marathon, California International Marathon, and felt extremely encouraged by the positive culture that is prevalent here. I felt important working on a real project that is deployed in production and actually has an impact on Strava. I appreciated the iterative development that allowed me to grow and become a better developer and a better person.</p><p><em>I would like to thank Jeff Pollard for being my technical mentor, Steve Lloyd for being my manager, and all of the Infrastructure team for supporting me throughout my time here. Go Terps.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fda64bed0504" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/upgrading-kafka-at-strava-fda64bed0504">Upgrading Kafka at Strava</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rebuilding the Segment Leaderboards Infrastructure: Part 4: Accessory Systems]]></title>
            <link>https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-4-accessory-systems-5e98dc9a3d78?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/5e98dc9a3d78</guid>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[scala]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[kafka]]></category>
            <dc:creator><![CDATA[Jeff Pollard]]></dc:creator>
            <pubDate>Tue, 28 Nov 2017 18:01:01 GMT</pubDate>
            <atom:updated>2017-11-28T18:01:01.492Z</atom:updated>
            <content:encoded><![CDATA[<p>Over the past year, the Strava platform team has been working to rebuild the segment leaderboards system. This is the final article in a series of four blog posts detailing that process, and describes how the leaderboards event stream architecture enables simple and easy extensions to the core functionality. For some added context, please read <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-1-background-13d8850c2e77">part one</a>, which details the background of the leaderboards system, <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-2-first-principles-of-a-new-system-cd2e77c82ba3">part two</a>, which distilled the problems of the previous leaderboards systems down to a set of principles a new system should solve, and <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-3-design-of-the-new-system-39fdcf0d5eb4">part three</a>, which describes details of the core leaderboards system.</p><h3>Accessory Systems</h3><p>As a quick refresher, part three of this series described a leaderboard architecture in which effort mutations in the Ruby on Rails app were logged to a Kafka topic partitioned by segment and user. Those mutations are consumed by a worker which refreshed effort data from the canonical effort store, and then applied the resulting updates to leaderboards storage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nMKEl-8XGLyn-nBi." /><figcaption><em>As a review, here is the existing architecture translating effort mutations into leaderboard updates. Note the “Leaderboards Service API,” which is an RPC service clients make requests through to query the Cassandra leaderboards storage.</em></figcaption></figure><p>While this system is the core of our final <em>leaderboards</em> system, it is far from the entire surface area of the leaderboards <em>service</em> at Strava. In addition to basic leaderboard functionality, the leaderboards service is also responsible for:</p><ol><li>Awarding top ten <strong>achievements</strong> (including KOM/QOM) from efforts on an activity.</li><li>Maintaining <strong>counts</strong> of the total number of efforts ever on a segment, and the total number of unique users who have attempted a segment.</li><li>Serving as a base store for dynamically <strong>filtered leaderboards</strong> (for example: a leaderboard composed only of the other users you are following, or the members of a club).</li></ol><p>A naive way to support these features would be to execute queries against Cassandra leaderboard storage:</p><ol><li><strong>Achievements</strong> — whenever we render an activity’s efforts within the Strava product, query the leaderboard for every segment traversed by that activity to determine if any of the activity’s effort resulted in a top ten.</li><li><strong>Counts</strong> — whenever we show a leaderboard, fetch all rows from it to determine the count.</li><li><strong>Filtered leaderboards </strong>— Whenever we need to show a dynamically filtered leaderboard, fetch all rows from the leaderboard and filter them for users which meet the criteria.</li></ol><p>While straightforward, querying Cassandra every time is incredibly costly — you may have noticed the phrase “fetch all rows” more than once in the previous paragraph. Full Cassandra table scans are computationally expensive, and time consuming. If we attempted these naive query patterns during regular query load, we would overwhelm our infrastructure and cause undesirable and unpredictable latency for requests.</p><p>The next logical step, then, is to denormalize the data these features require into a <em>derivative</em> datasets. These datasets contain precomputed answers to the common questions the above queries aim to answer, giving us predictable performance and predictable latency for read requests.</p><h3>Replication and Synchronization</h3><p>Reflecting back on the three features our leaderboards service needs to support, we can classify each access pattern into one of three derivative data sets:</p><ol><li><strong>Leaderboard Cache</strong> — copy of an often-requested portion of all leaderboards, held in a low latency format (i.e. in-memory), offering quick reads.</li><li><strong>Achievements</strong> — store of efforts which are within the top 10 of a leaderboard.</li><li><strong>Counts</strong> — aggregate counts of total efforts and unique number of users seen per leaderboard.</li></ol><p>To keep derivative data stores like these in sync, we will want to update them as soon as the leaderboards themselves are updated. To achieve this goal, oftentimes engineers will do the simple and straightforward thing of adding lines of code in the upstream application to update derivative data stores as well. For example, to keep leaderboard aggregate counts in sync we would add code to increment the total effort counter every time a new effort is added or removed from a leaderboard.</p><p>This approach may sound straightforward at first, but there is a lot of hidden complexity:</p><ul><li>Latency in updating downstream data sets add latency to upstream processing.</li><li>Availability of the upstream processing is now tied to availability of the downstream data stores.</li><li>Upstream processing is now concerned with proper error-handling/retry/data consistency requirements of downstream data sets.</li><li>Added complexity of code, integration, and testing downstream updates within the upstream system.</li></ul><p>In nearly all cases, these complications can be avoided by instead architecting your system in a <a href="https://www.confluent.io/blog/making-sense-of-stream-processing/">stream processing</a> approach, where changes in the upstream system are asynchronously replicated to downstream ones. In this design, the upstream triggering system logs mutations it is making, then downstream services consume those updates and apply changes to their own data stores. In our implementation, the worker updating leaderboards simply logs <em>leaderboard </em>mutation messages as it updates leaderboards. Downstream clients consume those mutations to keep their derivative data stores in sync.</p><h3>Derivative Data Sets</h3><p>In the rest of this article we will summarize how the leaderboards stream processing architecture allowed us to build three derivative data sets. We’ll slowly build out a diagram of the entire leaderboard architecture, showing how all three of the systems fit together as part of the larger whole.</p><h4>Leaderboard Cache</h4><p>While Cassandra meets our use case for leaderboard storage quite nicely, due to its data model, it has a limited querying ability. You are not able to craft queries as expressive as those in more traditional RDBMS (such as MySQL). The normal approach to this limitation is to predefine query patterns and denormalize data into separate Cassandra tables, one for each query pattern.</p><p>This technique is somewhat challenging for leaderboards. For the dynamic leaderboards, it is prohibitively expensive to materialize all of them in Cassandra. We would be looking at an additional leaderboard per segment for each user that has traversed the segment and each club in which a member has traversed the segment, together ordering on the tens or hundreds of millions. Thus, we have to fetch all efforts from the base leaderboard, and filter them in memory. These are not common requests, but do happen with enough frequency, and are expensive enough to Cassandra, that we wanted to look at ways to limit them to some degree.</p><p>Additionally, aside from paginated leaderboard results, the other main query we must fulfill is calculating a user’s ranking on a given leaderboard. Since the ranking is dependent on the number of rows above the user in a given leaderboard, we have to fetch all efforts that are faster than the user’s entry to determine the rank. This could potentially be the entire leaderboard (if the user is in last place), another very expensive operation.</p><h4>Caching</h4><p>The solution to this problem was to put a cache in front of Cassandra to serve common leaderboard requests. This cache would hold a subset of frequently requested leaderboards, stored in a data structure to facilitate quick response times. The cache should be able to serve leaderboard requests faster than Cassandra, and could potentially provide a richer querying API to handle those requests without needing a full scan.</p><h4>Populating the Cache</h4><p>Populating the cache should be done on a cache <em>miss</em> as seen from leaderboard read requests. On a miss, the client logs a message into a Kafka topic noting the leaderboard which had the cache miss. A downstream consumer (discussed shortly) consumes the cache miss message, and populates the cache by querying for canonical leaderboard data from Cassandra.</p><p>You may be wondering why we didn’t craft the cache as a <a href="https://docs.oracle.com/cd/E16459_01/coh.350/e14510/readthrough.htm#sthref111">read through cache</a>, where the client queries Cassandra on a miss, and then caches that response as part of the initial read request. The reason is that we actually cache the entire leaderboard (all efforts), not just the specific response for the specific request. And we do not want to introduce latency to the read request to do this cache operation, so instead it is deferred to a out-of-band downstream consumer.</p><p>This may see counterintuitive, but it’s important to note that caching specific leaderboard responses is incredibly hard. Leaderboard responses include a ranking value for each effort returned. This is the effort’s ranking in the leaderboard, noting how many faster efforts there are before it. Caching partial sections of leaderboard standings with a ranking is dangerous — because as new efforts are added or removed from the leaderboard, the ranking value for <em>other efforts on the leaderboard </em>change. For example, if I upload a best effort on a segment with 10,000 athletes and I am the new 250th best effort, the remaining 9,750 efforts all need to have their ranking shifted down by one. If we cached portions of leaderboards, on any given best effort mutation we would have to invalidate all partial responses that had their standing changed by that effort. This is certainly <em>possible</em>, but incredibly challenging and untenable.</p><p>Caching, and then invalidating, the <em>entire</em> leaderboard on each new best effort mutation is certainly simpler and easier to reason about. Unfortunately, popular (read: large) leaderboards see tens of updates a day, lowering cache hit rate for that class of leaderboard. This is extra-unfortunate since those popular leaderboards are the slowest to query, and then need caching the most.</p><p>The ideal solution is to cache the whole leaderboard in a structure which maintains ranking on updates, but allows <em>partial</em> updates as new efforts are added and removed from the leaderboard. This is tricky to do, as cache population due to cache misses needs to be synchronized with these partial updates. If you process them independently, you could end up with inconsistent data. Imagine Cassandra is queried by the actor processing the cache miss, but before the actor can write the leaderboard to cache, an effort is added to that same leaderboard in Cassandra. This effort was added after the first actor queried Cassandra, but since the leaderboard doesn’t exist yet (we are waiting on that actor to process the miss), the separate actor processing the leaderboard update does not apply the change. The first actor processing the miss then writes the leaderboard to cache, and we’ve lost the leaderboard update.</p><p>To achieve this synchronization, we rely on our good friend Kafka topic partitioning. Both cache miss messages, along with leaderboard updates, are logged into the same topic partitioned this time by segment and leaderboard type. This ensures that all writes to a particular type of segment leaderboard — both populates <em>and</em> updates — are serialized, processed by no more than one actor at a time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R00uXe9DJP5MtMk_." /><figcaption>Adding cache support was as simple as logging both cache misses and leaderboard updates for the new cache worker to consume. The new cache worker components are highlighted in yellow.</figcaption></figure><h4>Cache Data Store</h4><p>What data store will work with this caching strategy? Basic object caches, like Memcache, are out, since they do not support partial updates or ordered data structures. What we need is a cache that has slightly richer features and data structures which support partial updates. As it turns out, Redis is a good candidate for this problem.</p><p>The Redis <a href="https://redis.io/topics/data-types#sorted-sets">sorted set</a> data structure is a natural fit, because it allows us to sort efforts by their elapsed time. However, due to details outside of the scope of this blog post, the actual implementation of leaderboards in Redis required keeping both a sorted set and a hash in order to efficiently implement all needed leaderboard queries. This represented a challenge, as Redis does not have any sort of transaction isolation, and we did not want to use locks, for reasons mentioned in <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-1-background-13d8850c2e77">earlier posts</a> in this series.</p><p>However, Redis does have the <a href="https://redis.io/commands/eval">ability to run Lua scripts</a> via an embedded Lua interpreter, and the entire Lua script runs from start to finish atomically on the Redis instance. Leveraging this fact, we were able to write a suite of Lua scripts which atomically update both the sorted set and hash atomically. The cache worker then simply executes these scripts when processing cache miss and leaderboard update messages.</p><p>Finally, it’s important to address part one of this series, where we noted Redis was a “bad choice” for storage. We’re less concerned with the poor Redis ops story in this deployment, as all cache data is ephemeral. If our Redis instance crashes, we will lose all cached data, but the leaderboards system will still have high availability — we can read from Cassandra temporarily until a replacement node is back online.</p><h3>Achievements</h3><p>To record top ten achievements, a worker consumes leaderboard mutation messages logged by the cache worker, filtering them out for only efforts on either the KOM or QOM (colloquially called XOM) leaderboards. For each XOM effort seen, we query the leaderboard it appears on, to see if the effort is in the top ten results. If it is, we enqueue a job to record the achievement in our Rails app, which owns the display of these achievements.</p><p>Why does the worker need to look up the top ten leaderboard results if we have the leaderboard mutation already? The answer is that the leaderboard mutation does not include the ranking of the user on the leaderboard. This was a conscious decision. While calculating such a rank is <em>possible</em> at effort add time to Cassandra, calculating it is prohibitively expensive as it requires counting all the rows for efforts with a elapsed time faster than the effort just inserted. Rather than slow down the workers updating Cassandra, we decided to do this work via a separate worker after the leaderboard has been updated. That downstream worker is then able to query for leaderboards like any other client (using the cache), and only query the top ten results: a much cheaper query than asking for the ranking of the effort on the whole leaderboard.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HWRvZ6RgZprX2Ut7." /><figcaption><em>Logging cache worker cache mutations works as a signal for the achievements worker to check the leaderboard for achievements. The new achievements worker components are highlighted in yellow.</em></figcaption></figure><p>The tradeoff with this approach is a small chance of missed achievements during a high volume of concurrent effort uploads to the same leaderboard. For instance, if I upload an effort that is the new KOM on a segment, but before the worker calculating achievements processes the leaderboard mutation, my friend uploads their effort which is even faster and takes over the KOM. <em>Technically</em> my effort was the KOM, for the brief moment between my effort upload and my friends. However, when achievements worker actually queries for the top ten based on my effort it would see my effort as <em>second</em> place, with my friends effort as the KOM.</p><p>This is certainly not ideal, but this situation is very rare. Even if it does occur, most users would be okay (or even in some cases, unaware) they held the XOM for that short amount of time. Additionally, even if they did know they held it, under normal operating conditions they’d have only held the XOM for a second or two, not long enough to really consider themselves the XOM holder with any authority.</p><h3>Aggregate Counts</h3><p>As mentioned above, we maintain two aggregate total effort counts on a segment: the number of efforts ever, and the number of distinct users. The old Scala-powered system updated both counts via addEffort or removeEffort calls. The total effort count was implemented as a denormalized Redis counter, while the distinct user count was simply the <a href="https://redis.io/commands/hlen">HLEN</a> of the redis hash per segment.</p><p>For the distinct user count, there is no equivalent O(1) HLEN command for Cassandra. We have to use the more expensive <a href="https://docs.datastax.com/en/cql/3.1/cql/cql_reference/select_r.html#reference_ds_d35_v2q_xj__counting-returned-rows">COUNT(*)</a> query on the overall the leaderboard. Since this is so much more expensive, we made the decision to denormalize effort counts into another data store, keyed by the segment. For every leaderboard mutation seen on the overall leaderboard, we issue a COUNT(*) command and write the result to the denormalized store. This will obviously calculate counts for every segment in the whole system, even for segments which no client ever queries for, but popular leaderboards see hundreds of requests for their count per day, so the tradeoff is worth it for predictable O(1) read performance.</p><p>Total effort counts are more complicated. We cannot simply increment/decrement counts as we consume effort mutation messages, because that operation is not idempotent. If the consumer crashes and picks up from an earlier offset in the log we will be double incrementing/decrementing efforts.<em> </em>It turns out this is a <a href="https://hughfdjackson.com/engineering/counters,-idempotence-and-forgetful-bloom-filters/">common problem in distributed systems</a>, with the distinction that we’re not really looking for a <em>counter</em>, really we want a <em>set</em> of all efforts on a segment, using the <em>cardinality</em> of that set as the total count.</p><p>However, sets are quite memory intensive; we would need to store every effort ever at Strava to produce accurate counts. This is also <a href="https://en.wikipedia.org/wiki/Count-distinct_problem">another common problem</a>, for which most solutions rely on <a href="https://dzone.com/articles/introduction-probabilistic-0">probabilistic data structures</a>. In our case, a <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> data structure works well for use case. It supports idempotent adds in O(1), with O(1) cardinality checking. So effort counts are now as simple as consuming effort mutations, adding each effort to a HyperLogLog per segment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*b-t5S4T5OAWq5B9y." /><figcaption><em>The existing leaderboard updates topic logged by the cache worker is also used to notify the counters worker to update the denormalized leaderboard counts. The new counters worker components are highlighted in yellow.</em></figcaption></figure><p>Unfortunately, as of today, this is not the way our production system maintains total effort counts. We still naively update counts via addEffort or removeEffort calls. As mentioned earlier, to accurately build a HyperLogLog counter for all efforts ever per segment we’d have to play all efforts ever into it. This is a non-trivial backfill operation, and instead we just decided to migrate the counters from the old infrastructure and maintain them the same way. This is obviously not ideal, but is no worse than the current implementation and allowed us to more quickly migrate away from the old leaderboards infrastructure.</p><h3>Conclusion</h3><p>Hopefully, after this article, it is clear that a stream processing architecture provides a great foundation to easily build services that produce derivative datasets. We saw it was easy to build several systems — a robust leaderboard cache containing a subset of often-requested leaderboards, a system to detect and publish top ten achievements, and a system to maintain denormalized aggregate counts.</p><p>While these datasets are updated asynchronously, with some latency, between their triggering actions upstream and an update reflected in the derivative data store, the latency is generally very low. This is a worthwhile tradeoff — in exchange for a latency hit, we get a system with increased reliability and consistency of updates, decoupling of components, scaling independence and flexibility, and simpler error and retry handling.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5e98dc9a3d78" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-4-accessory-systems-5e98dc9a3d78">Rebuilding the Segment Leaderboards Infrastructure: Part 4: Accessory Systems</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rebuilding the Segment Leaderboards Infrastructure — Part 3: Design of the New System]]></title>
            <link>https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-3-design-of-the-new-system-39fdcf0d5eb4?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/39fdcf0d5eb4</guid>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[cassandra]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[kafka]]></category>
            <dc:creator><![CDATA[Jeff Pollard]]></dc:creator>
            <pubDate>Tue, 07 Nov 2017 18:32:09 GMT</pubDate>
            <atom:updated>2017-11-07T18:32:29.658Z</atom:updated>
            <content:encoded><![CDATA[<p>Over the past year, the Strava platform team has been working to rebuild the segment leaderboards system. This is the third in a series of four blog posts detailing that process. This post describes the implementation details of the new leaderboards system, showing how it achieves the design goals set out in earlier posts, and fixes some common problems seen in the previous leaderboards system. For some added context, please read <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-1-background-13d8850c2e77">part one</a>, which details the background of the leaderboards system, and <a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-2-first-principles-of-a-new-system-cd2e77c82ba3">part two</a>, which distilled the problems of the previous leaderboards systems down to a set of principles a new system should solve.</p><h3>Goals Review</h3><p>Reflecting back on part two of this series, we have two systems we need to build out for the new leaderboards service:</p><ol><li>A <strong>stream processing system</strong>, which is strongly ordered, partitionable, and idempotent.</li><li>A <strong>data storage system</strong> which is distributed, able to store all leaderboards in a homogenous way, and supports fast writes.</li></ol><h3>Stream Processing System</h3><p>If you need a strongly ordered, partitioned backing store for messages in a stream processing system, <a href="http://kafka.apache.org/">Kafka</a> is the obvious choice. Out of the box, it offers replication, strong ordering, high availability, and partitioning — exactly the system properties we need. There are certainly other messaging solutions which could be considered; those run the gamut of everything from Redis (via PubSub), to more mature and pure messaging solutions such as ActiveMQ or RabbitMQ. These others messaging systems generally do not have the same durability, availability, or ordering guarantees as Kafka, and we already used Kafka in production and liked it, which made it a relatively easy choice from there.</p><p>If you’re not familiar with Kafka, I’d suggest spending some time reading the Kafka documentation, specifically the section on its <a href="http://kafka.apache.org/documentation/#design">design</a>. This will make the rest of this post easier to follow.</p><h4>Design</h4><p>With Kafka in hand, let’s figure out how to go from efforts being created, updated, or deleted in our Rails app, to an ordered, partitioned, stream of <em>add</em> and <em>remove</em> leaderboard update messages.</p><p>A naive solution may be to have the Rails application log the new effort state to a Kafka topic each time the effort is mutated. A downstream consumer could then simply consume those effort mutation messages, and apply updates to the data store as needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JMX_yz0zuiXPHhYO." /></figure><p>This seems like it would work, but there are consistency problems. While effort mutations are strongly ordered within the Kafka partition, we’re still at the mercy of the message <em>publishers</em> to log them to that partition in the correct order. As part two of this series points out, efforts are mutated in our Rails application in parallel (occurring in web transactions, background jobs, etc) without any sort of synchronization or ordering. Consequently, effort mutations can (and will) be published out of order, which renders the order of logged mutations more-or-less useless.</p><p>There is hope, though. One important property of our system is that for the entire lifecycle of an effort, two important fields are immutable: the <strong>segment</strong> the effort traversed, and the <strong>user</strong> whose activity the effort belongs to. Knowing this, we can instead use the effort mutation messages logged by active only as a “notification” that something about the effort changed. The message doesn’t describe what about that effort specifically changed, we only know <em>something</em> changed and the leaderboard store may need to be updated.</p><p>Under that model, we can push the ordering requirement further down into the leaderboards system, where we have more control. If the worker consumes the partition of effort mutation in order, as it processes each message it can query our canonical effort storage for the latest effort data, and then make a decision on what kind of update to apply to leaderboards at that point.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-7h2JM0peuXOzLwZ." /></figure><p><em>No matter what order the effort mutation messages were logged in, the worker will eventually update leaderboard storage to the correct data.</em></p><p>The key takeaway here is that this architecture is eventually consistent, and handles all race conditions and out of order logging of effort mutations from the Rails app. This is due to a couple important properties:</p><ol><li>The actual transaction mutating the row in the database for the effort commits <em>before</em> the effort mutation event is logged.</li><li>A correct leaderboard update can be determined based on the state found in canonical effort storage, for any possible mutation event.</li></ol><p>Even if the worker reads an effort row from canonical storage, and then another actor in the system immediately updates the effort, rendering the worker’s read stale and invalid, the worker can still safely apply its now stale update. This is safe because the worker will eventually consume the mutation logged by the actor due to its effort update. On consumption, the worker will then read the final correct effort state, and update leaderboards accordingly.</p><h4>Immutability</h4><p>Before declaring victory with the stream processing system, it’s also important to think about our other stream processing goal of idempotency. What would happen if we replayed a message more than once? Or, since we will be implementing our worker as a Kafka consumer, what happens if it crashes, and then restarts its consumption from an earlier offset in the log? Can leaderboard consistency be maintained?</p><p>Thankfully, yes. If we consider each effort mutation as a stateless notification the effort mutated in some way, then being re-notified of a mutation will simply query the latest state again. Our consumer will still attempt to issue the leaderboard update again, but update is based on the most recently queried state from our canonical effort storage. In the even more complicated example of our consumer crashing and restarting its processing from an earlier point in the log, the same property still holds. The worker will attempt to issue the leaderboard update again based on the latest data.</p><h4>Set Up for Success</h4><p>The end result of this design is that we have a system which turns a potentially out-of-order series of effort mutations into an ordered sequence of leaderboard updates. As long as sequential processing of a partition occurs, effort updates are idempotent, and rewindable, due to the strong ordering guarantees of a Kafka partition and the causal relationship of updates in the system.</p><p>Understanding the properties of messages in your system often times can reveal characteristics you are able to leverage for efficient and accurate processing. In particular, the leaderboards architecture sets us up nicely to use <a href="https://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> and <a href="https://martinfowler.com/bliki/CQRS.html">command query responsibility segregation</a> (CQRS) in building later components of the segments leaderboard system. We will discuss those in part four of this series.</p><h3>Data Store</h3><p>With our stream processing architecture figured out, it’s time to turn our attention to the data store. To review: in part two of this series we noted the desire for a store that was distributed and highly available, as well as being able to achieve a high write load, and store all leaderboards in a homogenous way.</p><p>Influencing our data store decision was the preexisting usage of Cassandra at Strava. We use Cassandra in a variety of other systems already, and it was a common opinion that it could serve as a good candidate for the leaderboards store. On the surface, Cassandra certainly met a lot of the project’s needs — it’s distributed, highly available, able to support a high level of writes, and had a strong <a href="https://www.datastax.com/dev/blog/we-shall-have-order">emphases on ordering</a>. Consequently, a lot of our work in choosing the leaderboards data store went into determining if Cassandra would be an ideal fit.</p><h4>Cassandra</h4><p>Our confidence with Cassandra mostly mostly hinged on two main questions:</p><ol><li>Is it possible to construct a suitable data model/table schema using Cassandra?</li><li>Will Cassandra’s eventual consistency work for our use case?</li></ol><h4>Table Schema</h4><p>Modern Cassandra schemas are generally defined in <a href="http://cassandra.apache.org/doc/latest/cql/">CQL</a>, a SQL-like abstraction over native Cassandra data structures. So it’s best to think of the Cassandra schema in terms of more traditional SQL-like rows and columns.</p><p>Given our leaderboards use case, we crafted a schema we felt would work:</p><pre>CREATE TABLE leaderboards (</pre><pre>segment_id int,</pre><pre>leaderboard_type int, // overall, male, female, etc.</pre><pre>elapsed_time int,</pre><pre>effort_id bigint,</pre><pre>effort_data blob,</pre><pre>user_id int</pre><pre>)</pre><p>Each row in a Cassandra table is keyed by a unique <a href="http://cassandra.apache.org/doc/latest/cql/ddl.html#primary-key">PRIMARY KEY</a>. Choosing a PRIMARY KEY, along with Cassandra data modeling in general, is a complicated topic, but some distilled guidance is best summed up via <a href="https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling">a blog post by Datastax</a> as:</p><blockquote><strong>Rule 1: Spread Data Evenly Around the Cluster</strong></blockquote><blockquote>You want every node in the cluster to have roughly the same amount of data. […] Rows are spread around the cluster based on a hash of the partition key, which is the first element of the <em>PRIMARY KEY</em>.</blockquote><blockquote><strong>Rule 2: Minimize the Number of Partitions Read</strong></blockquote><blockquote>Partitions are groups of rows that share the same partition key. When you issue a read query, you want to read rows from as few partitions as possible.</blockquote><p>One final note: within a partition, rows are ordered by the remainder of columns in the PRIMARY KEY, known as the <a href="http://docs.datastax.com/en/archived/cql/3.0/cql/ddl/ddl_compound_keys_c.html">clustering key</a>.</p><p>Knowing this we might chose our PRIMARY KEY as:</p><pre><em>(segment_id, leaderboard_type, elapsed_time, timestamp, effort_id)</em></pre><p>Here, segment_id is the partition key, as it is the first element in the primary key. The remaining columns — leaderboard_type, elapsed_time, timestamp, effort_id— make up the clustering key, which defines the ordering of rows within the partition.</p><p>This seems great — with segment_id first in the PRIMARY KEY, all rows for a segment are partitioned onto the same node. In aggregate, this should spread all effort rows evenly around the cluster, with any one leaderboard request contained to a single partition. Within a partition, rows will be ordered by the leaderboard_type, and then by elapsed_time — just like we want for an actual leaderboard. This schema also allows us to store all leaderboards in a homogenous way, another one of the properties we wanted in our data store.</p><h3>Good for Reads, Bad for Writes</h3><p>While this schema works well for storing ordered sequences of efforts in a leaderboard, it is actually pretty <em>awful</em> at retrieving the effort for one particular user. This is a problem for our aforementioned worker consuming effort mutations. For any given <em>add</em> or <em>remove</em> message processed, our leaderboard service needs to query the leaderboard store for the user’s current best effort to compare it against the incoming effort. Due to the PRIMARY KEY we chose, we can’t efficiently issue that query. Cassandra only allows contiguous sequence of rows to be returned, and a particular user’s rows, for a segment, are interspersed amongst the ordered rows for all users for the segment.</p><p>We could maybe adjust the PRIMARY KEY to include the user_id:</p><pre>segment_id, leaderboard_type, user_id, elapsed_time, timestamp, effort_id</pre><p>This would make it trivial to query for an user’s best effort for that leaderboard_type, but now rows on disk are ordered by user_id, and not by elapsed_time like we want.</p><p>So what do we do? We looked at this and evaluated two options.</p><p>First, we considered having two separate tables. One table would have user_id in its PRIMARY KEY to allow efficient querying by user. Then, once that table had been updated, we would write the results to a <em>second</em> table<em>,</em> which had <em>its</em> PRIMARY KEY ordered for a leaderboard. This certainly would work, but keeping two tables in sync like this is tricky, and opens us up to a whole host of processing complications, edge cases, and failures you need to reason about and account for.</p><p>Second, we looked at using a Cassandra feature called <a href="https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html">secondary indexing</a>. Secondary indexes in Cassandra are a way to add an index on, and thus filter by, a column that is not part of the clustering key. There are <a href="https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes">many</a> <a href="https://dzone.com/articles/cassandra-indexing-good-bad">people</a> <a href="https://www.quora.com/What-is-secondary-index-in-Cassandra/answer/Christopher-Smith?srid=zj4">online</a> who discourage the use of secondary indexing. Their objections are based mostly on the fact that while secondary indexes have plenty of use cases where they seem to solve a problem, due to their implementation they <a href="https://docs.datastax.com/en/cql/3.3/cql/cql_using/useWhenIndex.html#useWhenIndex__when-no-index">do not actually work that well</a>. Additionally, they will slow down writes, as you need to update both the index and table on each write operation.</p><p>However, if you follow the guidance of the Cassandra documentation, understand your use case to ensure it fits, and dutifully benchmark your implementation, secondary indexes can work well. And thankfully, our data and query pattern was a good fit for a secondary index on the user_id column, so we created a test schema, benchmarked it, and determined that it would actually work well for us.</p><p>There is still one remaining problem with our Cassandra schema. Even with the index on user_id, determining the <em>ranking</em> of a user on a leaderboard is not straightforward. To answer this question, we need to know which number row, in order, the user is from the top of the leaderboard. There is no CQL operation that we can use to achieve this in an efficient way, so we have to query for <em>all</em> rows in the table, and manually determine the user ranking ranking in memory. This is not so bad for small leaderboards of a few hundred users, but many leaderboards are in the high tens of thousands of users in size. The cost of moving that amount of data across the network, combined with the computational cost of processing that many results is likely to be untenable.</p><p>This type of query is not handled well by most data stores, so we’re not really missing a whole lot by using Cassandra, but the situation is certainly less than ideal. To combat this problem, we’ll want to ensure that we both minimize the number of queries of this type as well as cache leaderboard data in such a wayas to make this class of queries more efficient. Stay tuned for part four of this series, where we will discuss this problem and the caching system in more detail.</p><h3>Consistency Model</h3><p>The other big consideration when using Cassandra is understanding its <a href="http://docs.datastax.com/en/archived/cassandra/3.x/cassandra/dml/dmlDataConsistencyTOC.html">consistency</a> model. Cassandra operates under last write wins (LWW) consistency, where each INSERT or UPDATE (really, <a href="https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlWriteUpdate.html">an </a><a href="https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlWriteUpdate.html">UPSERT</a>) is blindly appended to the database, without checking if a duplicate record exists. Then, during reads, if Cassandra encounters two versions of the same row, it chooses the newer one. “Newer,” defined as the row with the latest timestamp.</p><p>The “timestamp” a row is written with can be <a href="https://docs.datastax.com/en/cql/3.1/cql/cql_reference/insert_r.html">used-defined</a>, but generally is the current timestamp. This may sound like a scary way to do conflict resolution (there are <a href="https://aphyr.com/posts/299-the-trouble-with-timestamps">plenty of ways it can break</a>), but it’s incredibly fast and easy to reason about. In any running system you will likely see a small degree of data inconsistency, but nearly always that is a worthwhile tradeoff to the benefits of availability, replication and performance.</p><p>There are also ways to lower the rate of data inconsistency. Cassandra allows <a href="http://docs.datastax.com/en/archived/cassandra/3.x/cassandra/dml/dmlAboutDataConsistency.html">tunable consistency</a> parameters, which you can use to ensure all read and write operations are executed on a quorum number of nodes. Doing so will hurt both read/write latency and availability (as a larger number of machines need to be healthy in order to complete the request), however, you can use this technique to prioritize consistency over availability if the performance hit is manageable for your application. We also run Cassandra configured with 3 replicas of data (<em>r</em> value of 3), allowing us to tolerate the loss of one node and still successfully complete reads and writes on a quorum number of nodes (2). Granted if we lose 2 nodes we will stop serving requests that require those nodes to establish a quorum, but losing two nodes is unlikely to happen in isolation without a more systemic widespread networking issue affecting many parts of the system.</p><p>The leaderboard system also structures its updates to reduce the likelihood of conflicting updates. Since leaderboard updates are partitioned by segment/user, the likelihood of conflicting updates is lowered dramatically, as no other actor in the system can be reading or writing the same row at the same moment. There are certainly scenarios where this fact could become false — i.e. a consumer pauses (perhaps due to GC) during message processing, a replacement consumer is started, and then the original consumers resumes processing. Generally these types of situations are incredibly rare, or at least rare enough that the engineering work and system performance hit incurred by engineering to prevent them are not worth the small amount of data inconsistency seen.</p><p>After some more benchmarking, testing, and dual writing leaderboard entries to Cassandra, as well as the old system, we eventually determined Cassandra would be a suitable data store for the leaderboards system.</p><h3>Conclusion</h3><p>Our leaderboard system is composed of two main components:</p><ol><li>A <strong>Kafka stream processing system</strong>, which consumes effort mutation messages logged from the Rails application, fetches the latest effort data from canonical storage, determines an update to apply, and then applies it to leaderboard storage.</li><li>A <strong>Cassandra leaderboard store</strong> made up of a single CQL table keyed by segment and leaderboard type, with a secondary index on user_id to facilitate querying by user.</li></ol><p>These two systems work in conjunction as a pipeline to transform mutations of efforts by our Rails application into a denormalized Cassandra index of the best effort for any given segment/user/leaderboard combination. This index can be used to serve leaderboard read requests quickly and efficiently, with some known drawbacks and deficiencies that can be solved through strategic caching.</p><p>While this final architecture may seem simple or obvious, arriving at it took many weeks of analysis, prototyping, and benchmarking. This article also skipped over a decent amount of extra functionality and implementation details of the final production system. Some of that detail will be discussed in part four of this series, where we will review accessory systems built in conjunction with leaderboards, such as a leaderboard aggregate counters and a caching system, as well as enumerating additional benefits of this architecture.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=39fdcf0d5eb4" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/rebuilding-the-segment-leaderboards-infrastructure-part-3-design-of-the-new-system-39fdcf0d5eb4">Rebuilding the Segment Leaderboards Infrastructure — Part 3: Design of the New System</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building the Global Heatmap]]></title>
            <link>https://medium.com/strava-engineering/the-global-heatmap-now-6x-hotter-23fc01d301de?source=rss----89d4108ce2a3---4</link>
            <guid isPermaLink="false">https://medium.com/p/23fc01d301de</guid>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[maps]]></category>
            <category><![CDATA[strava]]></category>
            <category><![CDATA[spark]]></category>
            <dc:creator><![CDATA[Drew Robb]]></dc:creator>
            <pubDate>Wed, 01 Nov 2017 09:01:01 GMT</pubDate>
            <atom:updated>2018-04-04T20:42:01.839Z</atom:updated>
            <content:encoded><![CDATA[<p>Our Global Heatmap is the largest, richest, and most beautiful dataset of its kind. It is a visualization of two years of trailing data from Strava’s global network of athletes. To give a sense of scale, the new heatmap consists of:</p><ul><li>700 million activities</li><li>1.4 trillion latitude/longitude points</li><li>7.7 trillion pixels rasterized</li><li>5 terabytes of raw input data</li><li>A total distance of 16 billion km (10 billion miles)</li><li>A total recorded activity duration of 100 thousand years</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XUmHPb5Zd5LmgQ2tpkE8Tw.jpeg" /><figcaption>The <a href="https://labs.strava.com/heatmap/#12.25/37.68045/55.71434/hot/run">Global Heatmap in Moscow, Russia</a> demonstrating Mapbox GL’s rotate/pitch feature.</figcaption></figure><p>Beyond simply including more data, a total rewrite of the heatmap code permitted major improvements in rendering quality. Highlights include twice the resolution, rasterizing activity data as paths instead of as points, and an improved normalization technique that ensures a richer and more beautiful visualization.</p><p>The heatmap is now available on <a href="https://www.strava.com/heatmap">Strava</a> and the <a href="https://www.strava.com/routes/new">Strava Route Builder</a>. The rest of this post is a technical deep dive on the details of this update.</p><h4><strong>Background</strong></h4><p>From 2015 to 2017, there were no updates to the Global Heatmap due to two engineering challenges:</p><ul><li>Our previous heatmap code was written in low-level C code and was only designed to be run on a single machine. It would have taken months for us to to update the heatmap with this restriction.</li><li>Accessing stream data required one S3 get request per activity, so reading the input data would have cost thousands of dollars and been challenging to orchestrate.</li></ul><p>The heatmap generation code has been fully rewritten using <a href="https://spark.apache.org/">Apache Spark</a> and Scala. The new code leverages <a href="https://medium.com/strava-engineering/from-data-streams-to-a-data-lake-b6ca17c00a23">new infrastructure enabling bulk activity stream access</a> and is parallelized at every step from input to output. With these changes, we have fully conquered all scaling challenges. The full global heatmap was built across several hundred machines in just a few hours, with a total compute cost of only a few hundred dollars. Going forward, these improvements will enable updating the heatmap on a regular basis.</p><p>The remaining sections in this post describe in some detail how each step of Spark job for building the heatmap works, and provides details on the specific rendering improvements that we have made.</p><h4><strong>Input Data and Filtering</strong></h4><p>The raw input activity streams data comes from a Spark/S3/Parquet data warehouse. Several algorithms clean up and filter this data.</p><p>Most importantly, the heatmap only contains public activities and respects all privacy settings. Go <a href="https://blog.strava.com/press/heatmap-updates/">here</a> to learn more.</p><p>Additional filters remove erroneous data. Activities at higher than reasonable running speeds are excluded from the running heat layer because they are most likely mislabeled. There is also a higher speed threshold for bike rides to filter data from cars and airplanes.</p><p>The intent of the heatmap is to only show data from movement. A new algorithm does a much better job of classifying stopped points within activities. If the magnitude of the time averaged velocity of an activity stream gets too low at any point, subsequent points from that activity are filtered until the activity breaches a specific radius in distance from the initial stopped point.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*T5gmUQcHejlp7uicGA4LXA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*rvl_pz1heZYx-qYfivp1CA.png" /><figcaption>Comparison of rendering before (left) and after (right) artificial noise was added to disrupt artifacts from devices that correct GPS data to the nearest road.</figcaption></figure><p>Many devices (most notably the iPhone) will sometimes “correct” the GPS signal in urban areas by snapping it to known road geometry rather than a raw position. This can lead to an ugly artifact where heat is only one pixel wide along some streets. We now correct for this by adding a random offset (drawn from a 2 meter normal distribution) to all points for every activity. This level of noise is large enough to be effective at preventing this artifact without noticeably blurring other data.</p><p>We also exclude any “virtual” activities, such as Zwift rides, because these activities contain fake GPS data.</p><h4><strong>Heat Rasterization</strong></h4><p>After filtering, latitude/longitude coordinates of all data points are translated into <a href="http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/">Web Mercator Tile</a> coordinates at zoom level 16. This zoom level consists of a tessellation of the world into 2¹⁶ by 2¹⁶ tiles each consisting of 256 by 256 pixels.</p><p>The old heatmap rasterized each GPS point as exactly one pixel. This simple approach was often hindered because activities can record at most one data point per second. This frequently caused visible artifacts in areas of low heat because the speed and record rate of activities often led to multi pixel gaps . It also caused a bias in heat where activities generally move slower (consider uphill vs downhill). Since the new heatmap renders an additional zoom level (pushing the highest spatial resolution of a single pixel from 4 meters to 2 meters), the problem was made even more visible. The new heatmap instead draws each activity as a pixel-perfect path connecting subsequent GPS points. On average, the length of a segment between two points at zoom level 16 is 4 pixels, so this change is very noticeable.</p><p>To accomplish this in a parallel computation, we needed to handle the case where adjacent points in an activity map to different tiles. Each such pair of points is resampled to include intermediate points along the original line segment at the boundary of each tile crossed. After this resampling, all line segments then start and end on the same tile, or have zero length and can be omitted. Thus we can represent our data as <em>(Tile, Array[TilePixel])</em> tuples, where <em>Array[TilePixel]</em> is a continuous series of tile coordinates that describe the path of a single activity over a tile. This dataset is then grouped by tile, so that all of the data needed to draw a single tile is mapped to a single machine.</p><p>Each sequential pair of tile pixel coordinates is next rasterized onto a tile as a line segment using <a href="https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm">Bresenham’s line algorithm</a>. This line drawing step must be extremely fast, as it runs trillions of times. The tile itself at this stage is simply an <em>Array[Double](256*256)</em> representing the total count of lines to hit each pixel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kam77IsgZeipxAZ28WHrHg.gif" /><figcaption>Rendering comparison showing advantages of rasterizing paths over points, along with having more data. Location is <a href="http://labs.strava.com/heatmap/#15.00/-121.68887/43.99233/blue/all">Mt. Bachelor, Oregon</a>.</figcaption></figure><p>At the highest zoom level, we populate more than 30 million tiles. This poses a problem because directly holding every tile in memory as double arrays would require at least 30 million x 256 x 256 x 8 bytes = ~15TB of memory. While this amount of memory is not unreasonable to provision in a temporary cluster, it would be rather wasteful considering that the tiles can on average be highly compressed since the majority of entries are zero. For performance reasons, using a sparse array was not a viable solution. In Spark, we can greatly reduce the maximum memory requirement by ensuring that the parallelism of this stage is many times higher than the number of active tasks in the cluster. Tiles from a finished task are immediately serialized, compressed, and written to disk, so only the set of tiles corresponding to the active set of tasks need to be kept decompressed and in memory at any given time.</p><h4><strong>Heat Normalization</strong></h4><p>Normalization is the function that maps raw heat counts for each pixel after rasterization from the unbounded domain <em>[0, Inf)</em> to the bounded range <em>[0, 1]</em> of the color map. The choice of normalization has huge impact on how the heatmap appears visually. The function should be monotonic so that higher heat values are displayed as higher “heat”, but there are many ways to approach this problem. A single global normalization function would mean that only the single most popular areas on Strava would be rendered using the highest “hottest” color.</p><p>A slick normalization technique is to use the CDF (cumulative distribution function) of the raw values. That is, the normalized value of a given pixel is the percentage of pixels with a lower heat value. This method yields maximal contrast by ensuring that there are an equal number of pixels of each color. In photo processing, this technique is known as <a href="https://en.wikipedia.org/wiki/Histogram_equalization">Histogram equalization</a>. We use this technique with a slight modification to prevent quantization artifacts in areas of very low raw heat.</p><p>Computing a CDF only for the raw heat values in a single tile will not work well in practice because a map view typically shows at least 5x5 tiles (each tile is 256x256 pixels). Thus, we compute the joint CDF for a tile using that tile and all tile neighbors within a 5 tile radius. This ensures the normalization function can only vary at scales just larger than the size of a typical viewer’s screen.</p><p>To actually compute an approximation of the CDF that can be quickly evaluated, you simply sort the input data, and then take some amount of samples. We also found that it is better to compute a biased CDF by sampling more heavily towards the end of the array of samples. This is because in most cases, only a small fraction of the pixels have interesting heat data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VbDxEIAcJSm-ZpewPvvn4A.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*40tLAUJkkOYrXxmta_y6ww.jpeg" /><figcaption>Comparison of normalization methods (left: old, right: new) at 33% zoom to highlight normalization behavior. The new method allows for any range of raw heat data to be visible in a single image. Additionally, bi-linear interpolation of the normalization function between tiles prevents any visible artifacts at tile boundaries. Location is <a href="https://labs.strava.com/heatmap/#14/-122.45366/37.82582/hot/all">San Francisco Bay.</a></figcaption></figure><p>An advantage of this approach is that the heatmap has perfectly uniform distribution over the colormap. In some sense, this causes the heatmap to convey maximum information about relative heat values. We also subjectively find that it looks really nice.</p><p>A disadvantage of this approach is that the heatmap is not absolutely quantitative. The same color only locally represents the same level of heat data. We do offer a more sophisticated product <a href="https://strava.app.link/e/PQ60BdNgxH">Strava Metro</a> for planning, safety and transportation departments with a quantitative version of the heatmap.</p><h4><strong>Interpolate Normalization Functions Across Tile Boundaries</strong></h4><p>So far we have a normalization function for each tile that represents a CDF of pixels within the local neighborhood of a few tiles. However, the CDF will still discontinuously change at tile boundaries in a way that is noticeable as an ugly artifact, especially in areas where there is a large gradient of absolute heat.</p><p>To address this, we used bi-linear interpolation. Each pixel’s actual value is computed from a sum of its four nearest neighbor tile bi-linear coefficients given by <em>max(0, (1-x)(1-y))</em> where <em>x, y</em> are the distances from the tile’s center. This interpolation is expensive because we need to evaluate four CDFs for every pixel instead of one.</p><h4><strong>Recursion Across Zoom Levels</strong></h4><p>So far we have only talked about generating heat for a single zoom level. To process lower levels, the raw heat data is simply added together — four tiles become one tile with 1/4th the resolution. Then the normalization step is rerun as well. This continues until the lowest zoom level is reached (a single tile for the whole world).</p><p>It is very exciting when stages of a Spark job need to process an exponentially decreasing amount of data, and thus take an exponentially decreasing amount of time. After waiting about an hour for the first level stage to finish, it is a dramatic finish to see the final few levels take less than a second.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*2AAl8-Db33UhVNHsckYt4Q.gif" /><figcaption>Zooming out from a single tile in <a href="https://labs.strava.com/heatmap/#16/-0.15752/51.52894/all/hot">London, UK</a> to the entire world.</figcaption></figure><h4><strong>Serving It Up</strong></h4><p>We finally store normalized heat data for each pixel as a single <em>byte</em>. This is because we display that value using a color map, which is represented as a length 256 array of colors. We store this data in S3, grouping several neighboring tiles into one file to reduce the total number of S3 files required. At request time, the server fetches and caches the corresponding precomputed S3 meta-tile, then generates a PNG on the fly from this data using the requested colormap. Our CDN (Cloudfront) then caches all tile images.</p><p>We also made various front-end updates. We are now using <a href="https://www.mapbox.com/help/define-mapbox-gl/">Mapbox GL</a>. This allows for smooth zooming, as well as fancy rotation and pitch controls. We hope that you will enjoy exploring this updated <a href="https://www.strava.com/heatmap">heatmap</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=23fc01d301de" width="1" height="1"><hr><p><a href="https://medium.com/strava-engineering/the-global-heatmap-now-6x-hotter-23fc01d301de">Building the Global Heatmap</a> was originally published in <a href="https://medium.com/strava-engineering">strava-engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>
